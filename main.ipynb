{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721908c6",
   "metadata": {},
   "source": [
    "# Proyek Analisis Sentimen\n",
    "- **Nama:** Audy Nadira Ramadanti\n",
    "- **Email:** audynadiraramdanti@gmail.com\n",
    "- **ID Dicoding:** audy_nadira_ramadanti_zWZ9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645a7e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sastrawi in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (11.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: nlp-id in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.18.0)\n",
      "Requirement already satisfied: huggingface-hub==0.23.4 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlp-id) (0.23.4)\n",
      "Requirement already satisfied: nltk==3.9.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlp-id) (3.9.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlp-id) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn==1.5.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlp-id) (1.5.1)\n",
      "Requirement already satisfied: scipy==1.13.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlp-id) (1.13.1)\n",
      "Requirement already satisfied: wget==3.2 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlp-id) (3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.23.4->nlp-id) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.23.4->nlp-id) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub==0.23.4->nlp-id) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.23.4->nlp-id) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.23.4->nlp-id) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.23.4->nlp-id) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.23.4->nlp-id) (4.12.2)\n",
      "Requirement already satisfied: click in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->nlp-id) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->nlp-id) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->nlp-id) (2024.9.11)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.5.1->nlp-id) (3.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub==0.23.4->nlp-id) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub==0.23.4->nlp-id) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub==0.23.4->nlp-id) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub==0.23.4->nlp-id) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub==0.23.4->nlp-id) (2024.8.30)\n",
      "Requirement already satisfied: pyspellchecker in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (8.22.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\fx506\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\fx506\\appdata\\roaming\\python\\python311\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sastrawi\n",
    "!pip install wordcloud\n",
    "!pip install nlp-id\n",
    "!pip install pyspellchecker\n",
    "!pip install pandas scikit-learn gensim nltk\n",
    "!pip install tqdm\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5c50f",
   "metadata": {},
   "source": [
    "Pada proses ini, saya menginstal serangkaian library tambahan yang diperlukan untuk mendukung analisis data dan pemrosesan bahasa, terutama dalam konteks pengolahan teks dalam bahasa Indonesia. Proses dimulai dengan perintah *!pip install sastrawi* yang berfungsi untuk mengunduh dan memasang Sastrawi, library yang populer untuk stemming bahasa Indonesia. Selanjutnya, *!pip install wordcloud* menginstal library WordCloud yang membantu dalam visualisasi data teks melalui pembuatan awan kata, sehingga memudahkan dalam menganalisis frekuensi kata dari kumpulan data teks. Perintah berikutnya, *!pip install nlp-id*, memasang modul pemrosesan bahasa alami (NLP) khusus untuk bahasa Indonesia, yang berguna untuk tugas-tugas seperti tokenisasi dan analisis sintaksis. Selanjutnya, perintah *!pip install pyspellchecker* dipakai untuk menginstal PySpellChecker, sebuah library untuk pemeriksaan ejaan yang membantu meningkatkan akurasi data teks. \n",
    "\n",
    "Pada baris berikutnya, *!pip install pandas scikit-learn gensim nltk* menginstal beberapa library kunci sekaligus: *pandas* digunakan untuk manipulasi data berbentuk tabel (DataFrame), *scikit-learn* untuk tugas-tugas machine learning dan pemodelan, *gensim* untuk topik modeling dan representasi vektor kata, serta *nltk* yang menyediakan berbagai alat untuk analisis bahasa alami. Selanjutnya, *!pip install tqdm* diinstal untuk menghadirkan progress bar yang memudahkan pelacakan proses eksekusi kode yang memakan waktu. Terakhir, perintah *!pip install ipywidgets* memasang library ipywidgets yang memungkinkan penggunaan widget interaktif dalam Jupyter Notebook, sehingga interaksi dengan data dan visualisasi menjadi lebih dinamis. Secara keseluruhan, serangkaian instalasi ini memastikan bahwa seluruh komponen yang diperlukan untuk analisis teks dan penerapan metode NLP serta machine learning siap digunakan dalam lingkungan pengembangan Python saya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bdb20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FX506\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Pandas untuk manipulasi dan analisis data\n",
    "pd.options.mode.chained_assignment = None  # Menonaktifkan peringatan chaining\n",
    "import numpy as np  # NumPy untuk komputasi numerik\n",
    "seed = 0\n",
    "np.random.seed(seed)  # Mengatur seed untuk reproduktibilitas\n",
    "import matplotlib.pyplot as plt  # Matplotlib untuk visualisasi data\n",
    "import seaborn as sns  # Seaborn untuk visualisasi data statistik, mengatur gaya visualisasi\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datetime as dt  # Manipulasi data waktu dan tanggal\n",
    "import re  # Modul untuk bekerja dengan ekspresi reguler\n",
    "import string  # Berisi konstanta string, seperti tanda baca\n",
    "from nltk.tokenize import word_tokenize  # Tokenisasi teks\n",
    "from nltk.corpus import stopwords  # Daftar kata-kata berhenti dalam teks\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Stemming (penghilangan imbuhan kata) dalam bahasa Indonesia\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory  # Menghapus kata-kata berhenti dalam bahasa Indonesia\n",
    "from wordcloud import WordCloud  # Membuat visualisasi berbentuk awan kata (word cloud) dari teks\n",
    "import nltk  # Import pustaka NLTK (Natural Language Toolkit).\n",
    "nltk.download('stopwords')\n",
    "from nlp_id.lemmatizer import Lemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ede0c",
   "metadata": {},
   "source": [
    "Pada proses ini, sejumlah library Python diimpor untuk menyiapkan lingkungan yang komprehensif guna melakukan manipulasi data, analisis numerik, visualisasi, serta pengolahan bahasa alami (NLP) khususnya untuk bahasa Indonesia. Pertama, library *pandas* dan *numpy* dimuat untuk memudahkan manipulasi data tabel dan perhitungan numerik, dimana *pandas* memungkinkan analisis data yang fleksibel dan *numpy* menyediakan operasi matematis yang cepat, dengan seed yang diatur untuk memastikan reproduktibilitas hasil. Selanjutnya, *matplotlib* dan *seaborn* digunakan untuk membuat visualisasi data, di mana *seaborn* memberikan pengaturan visualisasi statistik yang lebih menarik. Modul-modul lain seperti *datetime*, *re*, dan *string* membantu dalam pengolahan data waktu, penggunaan ekspresi reguler, dan pengelolaan konstanta teks seperti tanda baca. Untuk memproses teks, fungsi tokenisasi dan daftar stopwords dari *nltk* disiapkan setelah melakukan pengunduhan kata-kata berhenti, sementara *Sastrawi* menyediakan alat untuk stemming dan penghilangan kata berhenti yang spesifik untuk bahasa Indonesia. Visualisasi teks lebih lanjut dilakukan dengan *WordCloud* yang menghasilkan awan kata dari kumpulan data teks. Selain itu, untuk memperkaya analisis NLP, library dari *nlp_id* digunakan untuk proses lemmatization, dan beberapa teknik representasi teks (CountVectorizer, TfidfVectorizer) serta model pembelajaran mesin (Word2Vec, RandomForestClassifier, MultinomialNB, LinearSVC) diimpor guna melakukan ekstraksi fitur dan klasifikasi teks. Di akhir pengaturan, *SpellChecker* dimanfaatkan untuk memperbaiki ejaan dan *train_test_split* dari *scikit-learn* disiapkan untuk membagi data ke dalam set pelatihan dan pengujian. Dengan rangkaian impor ini, seluruh komponen penting untuk analisis data dan pemrosesan teks telah disiapkan dalam lingkungan Python, sehingga memungkinkan analisis data yang mendalam dan terstruktur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0e7be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/codm_reviews.csv\")\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7353b",
   "metadata": {},
   "source": [
    "Pada proses ini, data ulasan yang sebelumnya telah disimpan dalam format **CSV** dibaca dan dimuat kembali ke dalam struktur **DataFrame** menggunakan fungsi `read_csv` dari pustaka **pandas**. File CSV tersebut berada di dalam direktori bernama **`data`** dengan nama file **`codm_reviews.csv`**. Dengan membaca file ini, seluruh isi CSV‚Äîbaik baris maupun kolom‚Äîdikonstruksi ulang ke dalam bentuk DataFrame, sehingga data menjadi lebih terstruktur dan siap untuk dianalisis atau dimanipulasi lebih lanjut menggunakan berbagai fungsi yang tersedia di pandas.\n",
    "\n",
    "Setelah data berhasil dimuat, fungsi `data.shape` digunakan untuk mengetahui dimensi dari DataFrame tersebut. Nilai yang dikembalikan adalah **(15000, 11)**, yang menunjukkan bahwa dataset terdiri dari **15.000 baris** dan **11 kolom**.  \n",
    "- Setiap **baris** mewakili satu entri atau satu ulasan dari pengguna aplikasi.  \n",
    "- Sementara itu, setiap **kolom** menyimpan atribut terkait ulasan tersebut, seperti ID ulasan, nama pengguna, isi komentar, skor rating, jumlah likes, tanggal ulasan, versi aplikasi, serta balasan dari pengelola aplikasi jika ada.\n",
    "\n",
    "Ukuran dataset ini tergolong **sedang**, sehingga cukup ideal untuk dilakukan eksplorasi data (EDA), visualisasi, maupun pelatihan model sederhana tanpa memerlukan sumber daya komputasi yang terlalu besar. Struktur data yang teratur juga mempermudah proses preprocessing maupun analisis lanjutan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191617d9",
   "metadata": {},
   "source": [
    "## Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690eb2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   reviewId              15000 non-null  object\n",
      " 1   userName              15000 non-null  object\n",
      " 2   userImage             15000 non-null  object\n",
      " 3   content               15000 non-null  object\n",
      " 4   score                 15000 non-null  int64 \n",
      " 5   thumbsUpCount         15000 non-null  int64 \n",
      " 6   reviewCreatedVersion  11258 non-null  object\n",
      " 7   at                    15000 non-null  object\n",
      " 8   replyContent          943 non-null    object\n",
      " 9   repliedAt             943 non-null    object\n",
      " 10  appVersion            11258 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9d5b7",
   "metadata": {},
   "source": [
    "Setelah data berhasil dimuat ke dalam DataFrame, fungsi `data.info()` digunakan untuk menampilkan informasi umum mengenai struktur dataset. Output dari fungsi ini memberikan gambaran penting mengenai jumlah entri, nama kolom, jumlah nilai non-null (tidak kosong) pada masing-masing kolom, tipe data, serta estimasi penggunaan memori oleh DataFrame.\n",
    "\n",
    "Berdasarkan hasil `data.info()`, diketahui bahwa dataset ini memiliki total **15.000 entri (baris)** dengan **11 kolom** yang menyimpan berbagai informasi terkait ulasan pengguna aplikasi. Berikut adalah beberapa poin penting yang dapat disimpulkan dari hasil tersebut:\n",
    "\n",
    "### 1. Kolom dan Tipe Data:\n",
    "- Dataset ini terdiri dari:\n",
    "  - **9 kolom bertipe `object`**, yang umumnya berisi teks atau string (misalnya: `reviewId`, `userName`, `content`, dan tanggal dalam format string).\n",
    "  - **2 kolom bertipe `int64`**, yaitu `score` dan `thumbsUpCount`, yang menyimpan data numerik berupa rating dan jumlah likes pada ulasan.\n",
    "\n",
    "### 2. Nilai yang Hilang (Missing Values):\n",
    "- Beberapa kolom memiliki **data yang lengkap (non-null)** untuk seluruh baris, seperti:\n",
    "  - `reviewId`, `userName`, `userImage`, `content`, `score`, `thumbsUpCount`, dan `at`.\n",
    "- Namun, ada juga kolom yang memiliki **nilai kosong (null)**:\n",
    "  - `reviewCreatedVersion` dan `appVersion`: hanya memiliki **11.258 nilai non-null**, yang berarti sekitar **3.742 baris tidak memiliki informasi versi aplikasi**.\n",
    "  - `replyContent` dan `repliedAt`: hanya memiliki **943 nilai non-null**, menandakan bahwa hanya sebagian kecil ulasan yang mendapatkan balasan dari pengelola aplikasi.\n",
    "\n",
    "### 3. Penggunaan Memori:\n",
    "- DataFrame ini menggunakan memori sekitar **1.3 MB**, tergolong ringan dan efisien untuk ukuran 15.000 entri. Meski demikian, pengelolaan memori tetap penting terutama saat melakukan pemrosesan lanjutan seperti transformasi atau analisis data skala besar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ebb0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e55544",
   "metadata": {},
   "source": [
    "Pada tahap ini, dilakukan proses pembersihan data untuk menghilangkan kolom-kolom yang mengandung nilai kosong (missing values) secara keseluruhan. Fungsi dropna() dari pustaka pandas digunakan untuk tujuan ini, dengan parameter axis=1 yang menunjukkan bahwa operasi penghapusan dilakukan berdasarkan kolom, bukan baris.\n",
    "\n",
    "Secara lebih spesifik, perintah data = data.dropna(axis=1) akan menghapus semua kolom dalam DataFrame yang memiliki setidaknya satu nilai kosong (NaN). Artinya, hanya kolom-kolom yang 100% lengkap (tidak memiliki nilai yang hilang) yang akan dipertahankan dalam DataFrame akhir.\n",
    "\n",
    "Dalam konteks dataset ini, kolom-kolom seperti reviewCreatedVersion, replyContent, repliedAt, appVersion, dan userName akan terhapus karena masing-masing mengandung sejumlah nilai kosong. Hasil akhirnya adalah DataFrame yang hanya terdiri dari kolom-kolom yang datanya utuh secara penuh.\n",
    "\n",
    "Langkah ini sering digunakan untuk menyederhanakan analisis awal dan menghindari potensi error akibat keberadaan data yang tidak lengkap. Namun, perlu diperhatikan bahwa pendekatan ini juga berisiko menghilangkan informasi yang sebenarnya penting, sehingga sebaiknya digunakan secara selektif tergantung pada tujuan analisis yang akan dilakukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510d00dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   reviewId       15000 non-null  object\n",
      " 1   userName       15000 non-null  object\n",
      " 2   userImage      15000 non-null  object\n",
      " 3   content        15000 non-null  object\n",
      " 4   score          15000 non-null  int64 \n",
      " 5   thumbsUpCount  15000 non-null  int64 \n",
      " 6   at             15000 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 820.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Menghapus baris duplikat dari DataFrame clean_df\n",
    "data = data.drop_duplicates()\n",
    " \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9e15a",
   "metadata": {},
   "source": [
    "Setelah melakukan pengambilan data, langkah selanjutnya adalah melakukan **pembersihan data dari entri duplikat** menggunakan fungsi `drop_duplicates()`. Tujuannya adalah untuk menghapus **baris-baris yang memiliki data identik di semua kolom**, sehingga setiap baris dalam DataFrame benar-benar merepresentasikan ulasan yang unik.\n",
    "\n",
    "Perintah `data = data.drop_duplicates()` secara otomatis mendeteksi dan menghapus baris-baris yang sama persis dengan baris lainnya. Ini adalah langkah penting dalam proses praproses data untuk **menghindari bias** dalam analisis atau pelatihan model machine learning akibat data yang berulang.\n",
    "\n",
    "Namun, berdasarkan hasil dari `data.info()`, diketahui bahwa jumlah entri tetap **15.000 baris**, yang berarti **tidak ditemukan adanya baris duplikat** dalam DataFrame tersebut. Dengan demikian, seluruh baris dalam data sudah bersifat unik sejak awal.\n",
    "\n",
    "Hasil akhir dari DataFrame terdiri dari **7 kolom**, yaitu:\n",
    "1. `reviewId` ‚Äì ID unik untuk setiap ulasan.  \n",
    "2. `userName` ‚Äì Nama pengguna yang memberikan ulasan.  \n",
    "3. `userImage` ‚Äì URL atau referensi gambar profil pengguna.  \n",
    "4. `content` ‚Äì Isi atau teks dari ulasan pengguna.  \n",
    "5. `score` ‚Äì Nilai rating yang diberikan (berkisar dari 1 hingga 5).  \n",
    "6. `thumbsUpCount` ‚Äì Jumlah tanda suka (like) yang diterima oleh ulasan.  \n",
    "7. `at` ‚Äì Tanggal dan waktu ulasan dibuat.\n",
    "\n",
    "Ukuran memori yang digunakan sekitar **820.4 KB**, yang mencerminkan data yang sudah relatif ringkas dan bersih. Langkah ini memastikan bahwa data siap digunakan untuk analisis lebih lanjut tanpa terganggu oleh entri duplikat atau redundansi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a15170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    " \n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333319c6",
   "metadata": {},
   "source": [
    "Dalam proses analisis teks, data yang bersumber dari ulasan pengguna biasanya mengandung banyak unsur yang tidak relevan atau tidak konsisten, seperti tanda baca, angka, simbol, bahkan bahasa campuran. Untuk itu, dilakukan beberapa tahapan pembersihan (preprocessing) guna mengubah teks mentah menjadi bentuk yang lebih bersih dan siap dianalisis. Berikut adalah penjelasan dari fungsi-fungsi preprocessing yang digunakan:\n",
    "\n",
    "### 1. `cleaningText(text)`\n",
    "Fungsi ini digunakan untuk membersihkan teks dari elemen-elemen yang tidak diperlukan. Proses yang dilakukan meliputi:\n",
    "- Menghapus **mention** (kata yang diawali `@`).\n",
    "- Menghapus **hashtag** (kata yang diawali `#`).\n",
    "- Menghapus teks **\"RT \"** yang sering muncul dalam retweet.\n",
    "- Menghapus **tautan/link** (yang diawali dengan `http`).\n",
    "- Menghapus **angka**.\n",
    "- Menghapus **karakter non-alfanumerik** (tanda baca dan simbol).\n",
    "- Mengganti karakter baris baru (`\\n`) dengan spasi.\n",
    "- Menghapus semua tanda baca dengan `translate`.\n",
    "- Menghapus spasi di awal dan akhir teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fbb50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db144e",
   "metadata": {},
   "source": [
    "### 2. `casefoldingText(text)`\n",
    "Fungsi ini mengubah seluruh huruf dalam teks menjadi **huruf kecil**. Tujuannya adalah untuk menyeragamkan bentuk kata sehingga analisis tidak terpengaruh oleh perbedaan kapitalisasi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f92f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2215fcad",
   "metadata": {},
   "source": [
    "### 3. `tokenizingText(text)`\n",
    "Fungsi ini memecah kalimat atau teks panjang menjadi daftar kata-kata (**token**), yang nantinya akan diproses lebih lanjut. Tokenisasi penting untuk menganalisis kata per kata dalam teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7aed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteringText(text): # Menghapus stopwords dalam teks\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\", \"nggak\", \"ngga\"])\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0f2d1",
   "metadata": {},
   "source": [
    "### 4. `filteringText(text)`\n",
    "Fungsi ini digunakan untuk menghapus **stopwords**, yaitu kata-kata umum yang biasanya tidak membawa makna penting dalam analisis (contoh: \"dan\", \"yang\", \"di\").  \n",
    "- Menggabungkan stopwords dari dua bahasa: **Indonesia dan Inggris**.\n",
    "- Menambahkan daftar kata tidak penting tambahan yang sering muncul dalam ulasan, seperti \"yaa\", \"gak\", \"nggak\", \"woi\", dll.\n",
    "- Hanya menyisakan kata-kata bermakna yang nantinya dapat digunakan dalam analisis sentimen atau topik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f16f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    # Membuat objek stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    " \n",
    "    # Memecah teks menjadi daftar kata\n",
    "    words = text.split()\n",
    " \n",
    "    # Menerapkan stemming pada setiap kata dalam daftar\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    " \n",
    "    # Menggabungkan kata-kata yang telah distem\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    " \n",
    "    return stemmed_text\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9203c86",
   "metadata": {},
   "source": [
    "### 5. `stemmingText(text)`\n",
    "Fungsi ini melakukan proses **stemming**, yaitu mengubah kata menjadi bentuk dasarnya (misalnya: \"bermain\" ‚Üí \"main\").  \n",
    "- Menggunakan pustaka `Sastrawi`, khusus untuk bahasa Indonesia.\n",
    "- Teks dipecah menjadi kata-kata, kemudian setiap kata dikembalikan ke akar katanya.\n",
    "- Setelah stemming, kata-kata digabungkan kembali menjadi sebuah string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "018809a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48df1a",
   "metadata": {},
   "source": [
    "### 6. `toSentence(list_words)`\n",
    "Fungsi ini digunakan untuk menggabungkan kembali daftar kata-kata hasil proses tokenisasi, filtering, atau stemming menjadi sebuah kalimat atau string tunggal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c3e2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def lemmatizeText(text):\n",
    "    lemmatized = lemmatizer.lemmatize(text)\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba9cd1",
   "metadata": {},
   "source": [
    "### 7. `lemmatizeText(text)`\n",
    "Fungsi ini menerapkan proses **lemmatisasi**, yang mirip dengan stemming namun mempertimbangkan konteks linguistik agar kata dikembalikan ke bentuk dasar yang benar secara gramatikal.  \n",
    "- Menggunakan objek `Lemmatizer()` (kemungkinan dari library `nlp-id` atau sejenisnya).\n",
    "- Dapat digunakan sebagai pelengkap stemming untuk meningkatkan akurasi dalam NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2948862",
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = {\"@\": \"di\", \"abis\": \"habis\", \"wtb\": \"beli\", \"masi\": \"masih\", \"wts\": \"jual\", \"wtt\": \"tukar\", \"bgt\": \"banget\", \"maks\": \"maksimal\", \"gk\": \"ga\", \"yg\": \"yang\", \"cod\": \"call of duty\", \"codm\": \"cll of duty mobile\", \"mvp\": \"most valuable player\", \"mp\": \"multi player\", \"gw\" : \"saya\", \"lu\":\"kamu\"}\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    " \n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    " \n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ced09c",
   "metadata": {},
   "source": [
    "Fungsi ini bertujuan untuk menangani slang words atau kata-kata tidak baku dan singkatan yang sering digunakan dalam percakapan informal, khususnya dalam ulasan pengguna aplikasi atau media sosial.\n",
    "\n",
    "Seringkali, pengguna menulis ulasan dengan menggunakan singkatan, istilah gaul, atau akronim seperti \"wtb\", \"bgt\", \"cod\", atau \"gw\". Jika tidak ditangani, kata-kata ini dapat mengganggu proses analisis teks karena tidak dikenali sebagai kata formal yang memiliki makna jelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37bdc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary(file_path):\n",
    "    \"\"\"Memuat kamus menjadi Counter dengan frekuensi kata\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        words = [word.strip().lower() for word in f]\n",
    "    return Counter(words)\n",
    "\n",
    "def correct_text(text, word_dict):\n",
    "    \"\"\"Mengkoreksi seluruh teks dengan memproses per kata\"\"\"\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', text)  # Memisahkan kata dan tanda baca\n",
    "    corrected = [correct_typo(word, word_dict) for word in words]\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "def edits1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word, word_dict):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in word_dict)\n",
    "\n",
    "def known(words, word_dict):\n",
    "    return set(w for w in words if w in word_dict)\n",
    "\n",
    "def correct_typo(word, word_dict):\n",
    "    \"\"\"Fungsi koreksi typo yang dimodifikasi untuk handle punctuation\"\"\"\n",
    "    if not word.isalpha():  # Pertahankan tanda baca/angka\n",
    "        return word\n",
    "    \n",
    "    word_lower = word.lower()\n",
    "    candidates = (known([word_lower], word_dict) or \n",
    "                 known(edits1(word_lower), word_dict) or \n",
    "                 known_edits2(word_lower, word_dict) or \n",
    "                 [word_lower])\n",
    "    \n",
    "    # Dapatkan kandidat dengan frekuensi tertinggi\n",
    "    return max(candidates, key=lambda x: word_dict.get(x, 0))\n",
    "\n",
    "\n",
    "word_dict = load_dictionary('data/kamus.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a7693",
   "metadata": {},
   "source": [
    "Fungsi ini bertujuan untuk memperbaiki kesalahan penulisan (typo) dalam teks ulasan yang sering muncul akibat ketidaksengajaan atau gaya penulisan informal dari pengguna.\n",
    "\n",
    "Kesalahan ejaan dapat memengaruhi efektivitas analisis teks, seperti klasifikasi kata, pencocokan token, dan hasil stemming. Oleh karena itu, fungsi ini mengimplementasikan metode koreksi ejaan berbasis _edit distance_ secara manual, tanpa menggunakan pustaka eksternal seperti `pyspellchecker`.\n",
    "\n",
    "Kamus referensi dimuat dari file teks (`kamus.txt`) dan dikonversi menjadi `Counter`, sehingga setiap kata memiliki frekuensi kemunculan yang bisa digunakan untuk memilih kandidat koreksi terbaik. Proses koreksi melibatkan:\n",
    "- Pencarian kata yang dikenal dari daftar (`known`)\n",
    "- Pembangkitan variasi kata hasil satu dan dua kali _edit_ dari kata yang salah (`edits1` dan `known_edits2`)\n",
    "- Pemilihan kata koreksi dengan frekuensi tertinggi dalam kamus sebagai hasil akhir\n",
    "\n",
    "Fungsi ini juga mempertahankan tanda baca dan angka agar tidak ikut dikoreksi, sehingga teks hasil koreksi tetap mempertahankan struktur aslinya.\n",
    "\n",
    "Kamus yang digunakan dalam proses ini dapat diperoleh dari repositori GitHub berikut:  \n",
    "üîó [https://github.com/kangfend/bahasa/blob/master/bahasa/data/kamus.txt](https://github.com/kangfend/bahasa/blob/master/bahasa/data/kamus.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bersihkan teks\n",
    "data['text_clean'] = data['content'].apply(cleaningText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389f2d7",
   "metadata": {},
   "source": [
    "Untuk mempersiapkan data teks sebelum dilakukan analisis lebih lanjut seperti *sentiment analysis*, dilakukan serangkaian tahapan *preprocessing* terhadap kolom `content` yang berisi teks ulasan. Tahapan-tahapan ini dilakukan secara bertahap dan terstruktur untuk menghasilkan teks yang bersih, seragam, dan bermakna. Berikut adalah tahapan lengkapnya:\n",
    "\n",
    "Langkah awal ini bertujuan untuk menghapus elemen-elemen yang tidak relevan dari teks, seperti mention, hashtag, tautan, angka, tanda baca, serta karakter-karakter khusus lainnya. Hasilnya adalah teks dalam bentuk yang lebih bersih dan netral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb303997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Case folding (huruf kecil)\n",
    "data['text_casefoldingText'] = data['text_clean'].apply(casefoldingText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b89499",
   "metadata": {},
   "source": [
    "Teks dibakukan ke dalam huruf kecil seluruhnya untuk menghindari perbedaan makna akibat kapitalisasi huruf, misalnya \"Main\" dan \"main\" akan dianggap sama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36c57e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Normalisasi kata slang\n",
    "data['text_slangwords'] = data['text_casefoldingText'].apply(fix_slangwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a874b2",
   "metadata": {},
   "source": [
    "Pada tahap ini, kata-kata gaul atau tidak baku seperti ‚Äúwtb‚Äù, ‚Äúgw‚Äù, ‚Äúbgt‚Äù, dll. diubah ke bentuk formal sesuai dengan kamus slang yang telah didefinisikan sebelumnya. Tujuannya agar kata-kata tersebut dapat dikenali dalam proses linguistik selanjutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856e9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 10420/15000 [1:46:59<30:07,  2.53it/s]  "
     ]
    }
   ],
   "source": [
    "# 4. Koreksi typo\n",
    "data['text_spellcorrect'] = data['text_slangwords'].progress_apply( lambda x: correct_text(x, word_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3edd6b",
   "metadata": {},
   "source": [
    "Langkah ini bertujuan untuk memperbaiki kesalahan penulisan (typo) pada setiap kata dalam teks menggunakan fungsi correct_text() yang telah dikembangkan sebelumnya. Fungsi ini tidak menggunakan pustaka eksternal seperti spellchecker, melainkan menerapkan logika koreksi ejaan berbasis edit distance dan referensi kamus lokal yang dimuat dari file kamus.txt.\n",
    "\n",
    "Setiap kata dalam teks akan dibandingkan dengan entri kamus, dan jika ditemukan kesalahan, kata tersebut akan dikoreksi menjadi kata yang paling mendekati serta memiliki frekuensi kemunculan tertinggi dalam kamus. Proses ini membantu meningkatkan kualitas teks untuk analisis selanjutnya seperti tokenisasi, stemming, dan klasifikasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81add02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Tokenisasi\n",
    "data['text_tokenizingText'] = data['text_spellcorrect'].apply(tokenizingText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df92c8",
   "metadata": {},
   "source": [
    "Teks kemudian dipisahkan menjadi daftar kata individual (token), sehingga setiap kata dapat dianalisis dan dimanipulasi secara terpisah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Stopword removal\n",
    "data['text_stopword'] = data['text_tokenizingText'].apply(filteringText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892fcf5f",
   "metadata": {},
   "source": [
    "Stopwords atau kata-kata umum yang tidak memiliki nilai semantik penting seperti ‚Äúyang‚Äù, ‚Äúdan‚Äù, ‚Äúitu‚Äù dihapus dari daftar token untuk menyisakan kata-kata yang lebih bermakna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Lemmatization\n",
    "data['text_lemmatized'] = data['text_stopword'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241c8bb",
   "metadata": {},
   "source": [
    "Setiap kata dalam token hasil sebelumnya dikembalikan ke bentuk dasarnya (lemma) dengan mempertimbangkan konteks linguistik. Misalnya: \"berlari\", \"berlari-lari\", \"lari-lari\" ‚Üí \"lari\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Gabung ke kalimat akhir\n",
    "data['final_text'] = data['text_lemmatized'].apply(toSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5eb8b",
   "metadata": {},
   "source": [
    "Langkah terakhir ini mengubah kembali daftar kata hasil lemmatization menjadi satu kalimat bersih yang siap untuk dianalisis, disimpan, atau digunakan dalam model machine learning.\n",
    "\n",
    "\n",
    "### **Hasil Akhir**\n",
    "Kolom `final_text` kini berisi teks ulasan dalam bentuk yang telah sepenuhnya diproses dan bersih, ideal untuk digunakan dalam tahap analisis berikutnya seperti klasifikasi sentimen, clustering, atau pembuatan word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d80cfe",
   "metadata": {},
   "source": [
    "Langkah ini saya gunakan untuk menampilkan lima baris pertama dari data ulasan yang telah diproses. Proses ini bertujuan untuk memberikan gambaran awal mengenai isi data setelah melalui serangkaian tahapan pembersihan dan transformasi teks.\n",
    "\n",
    "Melalui tampilan ini, kita dapat melihat bagaimana setiap ulasan pengguna telah mengalami proses normalisasi, mulai dari pembersihan teks, konversi ke huruf kecil, penggantian kata slang, koreksi typo, hingga proses tokenisasi, penghapusan stopwords, dan lemmatization. Hasil akhir dari seluruh proses ini ditampilkan dalam kolom final_text, yang berisi versi bersih dan siap analisis dari masing-masing ulasan.\n",
    "\n",
    "Dengan melihat sampel data awal ini, kita juga dapat mengevaluasi apakah tahapan preprocessing telah berjalan sesuai harapan sebelum melanjutkan ke tahap analisis selanjutnya seperti klasifikasi sentimen atau visualisasi teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentiment(score):\n",
    "    if score <= 2:\n",
    "        return 'Negatif'\n",
    "    elif score == 3:\n",
    "        return 'Netral'\n",
    "    else: \n",
    "        return 'Positif'\n",
    "\n",
    "# Terapkan ke data kamu\n",
    "data['sentiment'] = data['score'].apply(label_sentiment)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73496a27",
   "metadata": {},
   "source": [
    "Selanjutnya, proses ini merupakan proses pelabelan sentimen berdasarkan nilai skor yang diberikan oleh pengguna pada ulasannya. Setiap ulasan pada dataset memiliki atribut score yang berkisar dari 1 hingga 5. Skor ini kemudian dikategorikan ke dalam tiga kelas sentimen utama, yaitu:\n",
    "\n",
    "Negatif: untuk skor 1 dan 2, yang menunjukkan ketidakpuasan pengguna terhadap aplikasi.\n",
    "\n",
    "Netral: untuk skor 3, yang menunjukkan ulasan bersifat biasa saja, tidak terlalu puas maupun tidak puas.\n",
    "\n",
    "Positif: untuk skor 4 dan 5, yang menunjukkan kepuasan pengguna terhadap aplikasi.\n",
    "\n",
    "Hasil dari proses ini disimpan dalam kolom baru bernama sentiment, yang nantinya dapat digunakan sebagai label untuk keperluan analisis sentimen, pelatihan model klasifikasi, atau visualisasi distribusi opini pengguna terhadap aplikasi.\n",
    "\n",
    "Langkah ini penting karena mengubah skor numerik menjadi kategori yang bermakna secara emosional, sehingga lebih mudah untuk ditafsirkan dan dianalisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung jumlah masing-masing sentimen\n",
    "sentiment_counts = data['sentiment'].value_counts()\n",
    "\n",
    "# Cetak hasilnya\n",
    "print(\"Jumlah Positif:\", sentiment_counts.get('Positif', 0))\n",
    "print(\"Jumlah Netral :\", sentiment_counts.get('Netral', 0))\n",
    "print(\"Jumlah Negatif:\", sentiment_counts.get('Negatif', 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f1c85",
   "metadata": {},
   "source": [
    "Proses selanjutnya yaitu, saya menghitung jumlah ulasan berdasarkan kategori sentimen yang telah ditentukan sebelumnya, yaitu Positif, Netral, dan Negatif. Setelah setiap ulasan diberi label sentimen berdasarkan skor penilaian pengguna, proses ini dilakukan untuk mengetahui sebaran opini pengguna terhadap aplikasi.\n",
    "\n",
    "Dengan menggunakan fungsi value_counts(), kita dapat melihat berapa banyak ulasan yang termasuk dalam masing-masing kategori sentimen. Hasil ini memberikan gambaran umum mengenai tingkat kepuasan pengguna, serta menjadi dasar dalam melakukan analisis lebih lanjut atau visualisasi data.\n",
    "\n",
    "Berdasarkan hasil perhitungan:\n",
    "\n",
    "    - Jumlah ulasan positif sebanyak 154.931, menunjukkan sebagian besar pengguna merasa puas dengan aplikasi.\n",
    "\n",
    "    - Jumlah ulasan netral sebanyak 29.409, menunjukkan opini yang cenderung netral atau biasa saja.\n",
    "\n",
    "    - Jumlah ulasan negatif sebanyak 95.859, menandakan masih banyak pengguna yang memberikan ulasan dengan nada tidak puas.\n",
    "\n",
    "Informasi ini sangat penting untuk melihat bagaimana persepsi pengguna secara keseluruhan terhadap aplikasi, serta untuk mengidentifikasi potensi masalah atau kekuatan dari aplikasi berdasarkan data ulasan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'].value_counts().plot.pie(\n",
    "    autopct='%1.1f%%', \n",
    "    startangle=140, \n",
    "    colors=['#66bb6a', '#ef5350', '#ffee58'],  # Pastikan urutan warnanya cocok juga\n",
    "    labels=data['sentiment'].value_counts().index\n",
    ")\n",
    "plt.title('Proporsi Sentimen')\n",
    "plt.ylabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9731d71f",
   "metadata": {},
   "source": [
    "Selanjutnya saya **menyajikan distribusi kategori sentimen dalam bentuk visual pie chart**, yang bertujuan untuk memberikan gambaran yang lebih intuitif mengenai persebaran opini pengguna terhadap aplikasi.\n",
    "\n",
    "Dalam visualisasi ini, setiap bagian dari pie chart merepresentasikan **persentase ulasan dalam satu kategori sentimen** ‚Äî *Positif*, *Netral*, atau *Negatif*. Prosentase ditampilkan secara langsung di dalam grafik agar memudahkan interpretasi. Warna juga digunakan untuk membedakan masing-masing kategori:\n",
    "- **Hijau** untuk sentimen *Positif* (menunjukkan kepuasan),\n",
    "- **Merah** untuk *Negatif* (menandakan ketidakpuasan),\n",
    "- **Kuning** untuk *Netral* (opini yang bersifat netral atau tidak condong ke salah satu sisi).\n",
    "\n",
    "Berdasarkan hasil visualisasi:\n",
    "- Ulasan **positif** mendominasi dengan **55.3%**,\n",
    "- Diikuti oleh ulasan **negatif** sebanyak **34.2%**,\n",
    "- Dan **netral** sebesar **10.5%**.\n",
    "\n",
    "Visualisasi ini memperkuat pemahaman terhadap hasil analisis sentimen sebelumnya dan dapat dijadikan sebagai bahan presentasi atau pelaporan untuk menunjukkan persepsi publik terhadap aplikasi secara keseluruhan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386015fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat fungsi untuk generate WordCloud per label\n",
    "def generate_wordcloud(label):\n",
    "    text = ' '.join(data[data['sentiment'] == label]['final_text'])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"WordCloud - {label}\", fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "# Generate wordcloud untuk masing-masing sentimen\n",
    "for label in ['Positif', 'Netral', 'Negatif']:\n",
    "    generate_wordcloud(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c7911e",
   "metadata": {},
   "source": [
    "\n",
    "Terakhir saya **membuat visualisasi WordCloud berdasarkan kategori sentimen ulasan**, yaitu *Positif*, *Netral*, dan *Negatif*. Proses ini bertujuan untuk mengetahui **kata-kata yang paling sering muncul dalam ulasan dari masing-masing kelompok sentimen**, yang dapat memberikan wawasan mendalam mengenai isi opini pengguna.\n",
    "\n",
    "Pada tahap ini, data ulasan yang sudah melalui tahap pembersihan dan normalisasi (tersimpan di kolom `final_text`) digabungkan menjadi satu string besar untuk setiap kategori sentimen. Kemudian, digunakan library `WordCloud` untuk menghasilkan representasi visual dari frekuensi kata‚Äîsemakin sering sebuah kata muncul dalam ulasan, maka ukurannya akan tampak semakin besar di visualisasi.\n",
    "\n",
    "Visualisasi WordCloud ini dibuat untuk tiga kategori:\n",
    "- **Positif**: Menampilkan kata-kata yang sering digunakan oleh pengguna yang memberikan ulasan baik.\n",
    "- **Netral**: Menampilkan kata-kata umum yang tidak terlalu bernada positif atau negatif.\n",
    "- **Negatif**: Memperlihatkan kata-kata yang dominan dalam ulasan negatif, yang sering kali menunjukkan masalah atau keluhan.\n",
    "\n",
    "Melalui visualisasi ini, kita bisa **mengidentifikasi topik utama, persepsi pengguna, serta potensi masalah atau kekuatan dari aplikasi berdasarkan kata-kata yang paling dominan digunakan**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84dec5",
   "metadata": {},
   "source": [
    "## Ekstraksi Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeffa979",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['final_text']           # Teks akhir yang sudah dibersihkan\n",
    "X_token = data['text_lemmatized']  # List of token\n",
    "y = data['sentiment']            # Label sentimen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb1302",
   "metadata": {},
   "source": [
    "Masuk kedalam ekstraksi fitur, saya **memisahkan fitur (X) dan label (y) dari data yang telah dibersihkan**, sebagai bagian dari persiapan menuju tahap analisis lebih lanjut seperti klasifikasi sentimen.\n",
    "\n",
    "- **`X`** berisi data teks akhir (`final_text`) yang telah melalui seluruh tahapan preprocessing seperti pembersihan, normalisasi, koreksi typo, tokenisasi, penghapusan stopwords, lemmatization, dan penggabungan kembali ke bentuk kalimat. Teks ini digunakan sebagai **input utama** dalam proses pelatihan model.\n",
    "  \n",
    "- **`X_token`** adalah versi token dari data `X`, yaitu berupa daftar kata yang telah diproses dan dilemmatize, namun belum digabungkan menjadi kalimat. Ini bisa digunakan untuk analisis berbasis token, seperti perhitungan frekuensi kata atau pembuatan vektor dengan pendekatan berbasis token (contoh: Word2Vec atau TF-IDF berbasis token list).\n",
    "  \n",
    "- **`y`** adalah label target berupa kategori sentimen (*Positif*, *Netral*, atau *Negatif*) yang digunakan sebagai **output atau target** dalam proses supervised learning seperti klasifikasi sentimen.\n",
    "\n",
    "Langkah ini penting karena merupakan **awal dari tahap pemodelan machine learning atau NLP**, di mana data harus dipisahkan dengan jelas antara fitur dan target sebelum dilakukan pelatihan model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bdb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. BoW ---\n",
    "bow = CountVectorizer(max_features=200)\n",
    "X_bow = bow.fit_transform(X)\n",
    "X_bow_train, X_bow_test, y_bow_train, y_bow_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9cb93",
   "metadata": {},
   "source": [
    "Ekstraksi fitur pertama yang saya gunakan ialah  **Bag of Words (BoW)**. Pendekatan ini penting karena **model machine learning tidak dapat memahami teks dalam bentuk mentah**, sehingga diperlukan representasi numerik yang merepresentasikan informasi dari teks tersebut.\n",
    "\n",
    "Prosesnya sebagai berikut:\n",
    "\n",
    "- **`CountVectorizer`** digunakan untuk membentuk representasi BoW, yaitu dengan menghitung frekuensi kemunculan kata dalam dokumen. Di sini, hanya **200 kata paling sering muncul (berdasarkan `max_features=200`)** yang dipertahankan sebagai fitur. Ini dilakukan untuk **mengurangi kompleksitas dan memfokuskan pada kata-kata yang paling informatif**.\n",
    "  \n",
    "- Kemudian hasil transformasi disimpan dalam variabel `X_bow`, yang berisi representasi vektor dari masing-masing teks ulasan.\n",
    "\n",
    "- Setelah data vektor diperoleh, dilakukan proses **pembagian data menjadi data latih dan data uji** menggunakan `train_test_split`. Sebanyak **80% data digunakan untuk pelatihan model**, dan **20% sisanya digunakan untuk menguji performa model**. Proses ini penting untuk **mengukur kemampuan generalisasi model terhadap data yang belum pernah dilihat sebelumnya**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. TF-IDF ---\n",
    "tfidf = TfidfVectorizer(max_features=200, min_df=5, max_df=0.8)\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e50e01",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Selanjutnya saya membuat juga ekstraksi fitur dengan menggunakan  metode **TF-IDF**, yaitu sebuah teknik representasi teks yang mempertimbangkan **pentingnya sebuah kata dalam dokumen tertentu dibandingkan dengan seluruh dokumen dalam kumpulan data**.\n",
    "\n",
    "Berikut detail prosesnya:\n",
    "\n",
    "- **`TfidfVectorizer`** digunakan untuk mengonversi teks menjadi vektor TF-IDF, di mana setiap angka pada vektor merepresentasikan skor pentingnya sebuah kata di suatu dokumen.  \n",
    "- Saya membatasi jumlah fitur maksimal hanya **200 kata paling relevan** (`max_features=200`) untuk menjaga efisiensi komputasi.\n",
    "- Kata yang terlalu jarang muncul di dokumen (kurang dari 5 dokumen, `min_df=5`) diabaikan karena dianggap kurang informatif.\n",
    "- Begitu juga kata yang terlalu sering muncul di sebagian besar dokumen (lebih dari 80%, `max_df=0.8`) juga dihapus karena kemungkinan besar kata tersebut tidak memberikan makna khusus (*kata umum* seperti \"aplikasi\", \"game\", dll).\n",
    "  \n",
    "Setelah semua teks dikonversi menjadi bentuk vektor, data disimpan dalam `X_tfidf`, dan kemudian dibagi menjadi **data latih dan data uji** menggunakan `train_test_split`, dengan komposisi 80% untuk pelatihan model dan 20% untuk pengujian performa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23df614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Word2Vec ---\n",
    "model_w2v = Word2Vec(sentences=X_token, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def document_vector(doc):\n",
    "    doc = [word for word in doc if word in model_w2v.wv]\n",
    "    return np.mean(model_w2v.wv[doc], axis=0) if doc else np.zeros(model_w2v.vector_size)\n",
    "\n",
    "X_w2v = np.array([document_vector(doc) for doc in X_token])\n",
    "X_w2v_train, X_w2v_test, y_w2v_train, y_w2v_test = train_test_split(X_w2v, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1642f",
   "metadata": {},
   "source": [
    "## Modeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe50bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "print(\"===== RANDOM FOREST =====\")\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_bow_train, y_bow_train)\n",
    "print(f\"BoW     | Train: {rf.score(X_bow_train, y_bow_train):.4f} | Test: {rf.score(X_bow_test, y_bow_test):.4f}\")\n",
    "\n",
    "rf.fit(X_tfidf_train, y_tfidf_train)\n",
    "print(f\"TF-IDF  | Train: {rf.score(X_tfidf_train, y_tfidf_train):.4f} | Test: {rf.score(X_tfidf_test, y_tfidf_test):.4f}\")\n",
    "\n",
    "rf.fit(X_w2v_train, y_w2v_train)\n",
    "print(f\"W2V     | Train: {rf.score(X_w2v_train, y_w2v_train):.4f} | Test: {rf.score(X_w2v_test, y_w2v_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "print(\"\\n===== SVM =====\")\n",
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(X_bow_train, y_bow_train)\n",
    "print(f\"BoW     | Train: {svm.score(X_bow_train, y_bow_train):.4f} | Test: {svm.score(X_bow_test, y_bow_test):.4f}\")\n",
    "\n",
    "svm.fit(X_tfidf_train, y_tfidf_train)\n",
    "print(f\"TF-IDF  | Train: {svm.score(X_tfidf_train, y_tfidf_train):.4f} | Test: {svm.score(X_tfidf_test, y_tfidf_test):.4f}\")\n",
    "\n",
    "svm.fit(X_w2v_train, y_w2v_train)\n",
    "print(f\"W2V     | Train: {svm.score(X_w2v_train, y_w2v_train):.4f} | Test: {svm.score(X_w2v_test, y_w2v_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE BAYES\n",
    "print(\"\\n===== NAIVE BAYES =====\")\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_bow_train, y_bow_train)\n",
    "print(f\"BoW     | Train: {nb.score(X_bow_train, y_bow_train):.4f} | Test: {nb.score(X_bow_test, y_bow_test):.4f}\")\n",
    "\n",
    "nb.fit(X_tfidf_train, y_tfidf_train)\n",
    "print(f\"TF-IDF  | Train: {nb.score(X_tfidf_train, y_tfidf_train):.4f} | Test: {nb.score(X_tfidf_test, y_tfidf_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693da51f",
   "metadata": {},
   "source": [
    "## Tunning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contoh untuk salah satu representasi (misalnya TF-IDF)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Misal untuk TF-IDF\n",
    "grid_search.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best Score (CV):\", grid_search.best_score_)\n",
    "\n",
    "# Evaluasi di test set\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"TF-IDF | Test Accuracy: {best_model.score(X_tfidf_test, y_tfidf_test):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],  # 'l1' hanya boleh jika loss='squared_hinge' dan dual=False\n",
    "    'loss': ['squared_hinge'],\n",
    "    'dual': [True],  # dual=True untuk l2 + squared_hinge\n",
    "    'max_iter': [1000, 2000, 5000]\n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(\n",
    "    estimator=LinearSVC(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Misalnya kita tuning untuk TF-IDF dulu\n",
    "grid_search_svm.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "print(\"Best Params:\", grid_search_svm.best_params_)\n",
    "print(\"Best CV Score:\", grid_search_svm.best_score_)\n",
    "\n",
    "# Evaluasi model terbaik di test set\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "print(f\"TF-IDF | Test Accuracy: {best_svm.score(X_tfidf_test, y_tfidf_test):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
